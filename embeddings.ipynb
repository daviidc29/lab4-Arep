{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b539e86e",
   "metadata": {},
   "source": [
    "# Embeddings Lab \n",
    "## Name: David Santiago Castro\n",
    "\n",
    "This notebook reproduces the core preprocessing with embedding steps needed before training an LLM:\n",
    "- load raw text\n",
    "- tokenize into integer IDs\n",
    "- build input, target training samples with a sliding window max_length, stride\n",
    "- batch with a DataLoader\n",
    "- look up token embeddings and the typical positional embeddings idea\n",
    "- run a small experiment changing max_length and stride\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade72252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T02:06:22.518344Z",
     "iopub.status.busy": "2026-02-23T02:06:22.518054Z",
     "iopub.status.idle": "2026-02-23T02:06:26.246594Z",
     "shell.execute_reply": "2026-02-23T02:06:26.244434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0+cpu\n",
      "tiktoken available: True\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    _HAS_TIKTOKEN = True\n",
    "except Exception:\n",
    "    _HAS_TIKTOKEN = False\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"tiktoken available:\", _HAS_TIKTOKEN)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c38f6",
   "metadata": {},
   "source": [
    "## 1) Load raw text\n",
    "\n",
    "LLMs start from plain text. Everything later tokens, embeddings, training samples depends on the raw data:\n",
    "- what words/patterns exist\n",
    "- how often they appear\n",
    "- what contexts they appear in\n",
    "\n",
    "For agentic systems, this is also the starting point for building a knowledge base docs, logs, chat history before embedding/indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4a886e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T02:06:26.286146Z",
     "iopub.status.busy": "2026-02-23T02:06:26.285523Z",
     "iopub.status.idle": "2026-02-23T02:06:26.294647Z",
     "shell.execute_reply": "2026-02-23T02:06:26.293095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: the-verdict.txt\n",
      "Characters: 20479\n",
      "\n",
      "Preview (first 350 chars):\n",
      "\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glo\n"
     ]
    }
   ],
   "source": [
    "candidate_paths = [\"the-verdict.txt\", \"/mnt/data/the-verdict.txt\"]\n",
    "path = next((p for p in candidate_paths if os.path.exists(p)), None)\n",
    "assert path is not None, \"Could not find the-verdict.txt. Put it next to this notebook.\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Loaded:\", path)\n",
    "print(\"Characters:\", len(text))\n",
    "print(\"\\nPreview (first 350 chars):\\n\")\n",
    "print(text[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cc975",
   "metadata": {},
   "source": [
    "## 2) Tokenize to integer IDs\n",
    "\n",
    "Neural networks do math, not strings. Tokenization converts text into a sequence of integers.\n",
    "\n",
    "Why this matters:\n",
    "- the model predicts the next token over a fixed vocabulary\n",
    "- tokens define what the model can see and learn word pieces vs words, etc\n",
    "- tokenization impacts memory, speed, and how well rare words are handled\n",
    "\n",
    "We try to use tiktoken GPT‑style tokenization. If tiktoken can't download its encoding files, we fall back to a simple regex tokenizer that builds a vocab from this text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42d3e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T02:06:26.299311Z",
     "iopub.status.busy": "2026-02-23T02:06:26.298881Z",
     "iopub.status.idle": "2026-02-23T02:06:26.494617Z",
     "shell.execute_reply": "2026-02-23T02:06:26.492938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: tiktoken:gpt2\n",
      "Vocab size: 50257\n",
      "Total tokens: 5145\n",
      "\n",
      "First 30 token IDs: [40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285]\n",
      "Decoded back: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear\n"
     ]
    }
   ],
   "source": [
    "_SPLIT_RE = r\"(\\s+|[,.!?;:()\\[\\]{}])\"\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, s):\n",
    "        parts = re.split(_SPLIT_RE, s)\n",
    "        parts = [p for p in parts if p and not p.isspace()]\n",
    "        return [self.str_to_int.get(p, self.str_to_int[\"<|unk|>\"]) for p in parts]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = [self.int_to_str[i] for i in ids]\n",
    "        text = \" \".join(tokens)\n",
    "        text = re.sub(r\"\\s+([,.!?;:\\)])\", r\"\\1\", text)\n",
    "        text = re.sub(r\"\\(\\s+\", \"(\", text)\n",
    "        return text\n",
    "\n",
    "tokenizer_type = None\n",
    "\n",
    "if _HAS_TIKTOKEN:\n",
    "    try:\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        tokenizer_type = \"tiktoken:gpt2\"\n",
    "        token_ids = enc.encode(text)\n",
    "        vocab_size = enc.n_vocab\n",
    "        decode = enc.decode\n",
    "    except Exception as e:\n",
    "        print(\"tiktoken failed (likely offline). Falling back to SimpleTokenizer.\")\n",
    "        print(\"Reason:\", type(e).__name__, str(e)[:120], \"...\")\n",
    "        tokenizer_type = \"simple\"\n",
    "\n",
    "if tokenizer_type is None or tokenizer_type == \"simple\":\n",
    "    parts = re.split(_SPLIT_RE, text)\n",
    "    parts = [p for p in parts if p and not p.isspace()]\n",
    "    uniq = sorted(set(parts))\n",
    "    vocab = {tok: i for i, tok in enumerate(uniq)}\n",
    "    for special in [\"<|unk|>\", \"<|endoftext|>\"]:\n",
    "        if special not in vocab:\n",
    "            vocab[special] = len(vocab)\n",
    "\n",
    "    tok = SimpleTokenizer(vocab)\n",
    "    tokenizer_type = \"simple\"\n",
    "    token_ids = tok.encode(text)\n",
    "    vocab_size = len(vocab)\n",
    "    decode = tok.decode\n",
    "\n",
    "print(\"Tokenizer:\", tokenizer_type)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Total tokens:\", len(token_ids))\n",
    "\n",
    "print(\"\\nFirst 30 token IDs:\", token_ids[:30])\n",
    "print(\"Decoded back:\", decode(token_ids[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11a057",
   "metadata": {},
   "source": [
    "## 3) Sliding window samples (max_length, stride)\n",
    "\n",
    "Transformers train on fixed-length sequences  \n",
    "We turn one long token stream into many supervised examples:\n",
    "\n",
    "- X: tokens *t .. t+max_length-1*\n",
    "- Y: tokens *t+1 .. t+max_length*  (shifted by 1)\n",
    "\n",
    "Why overlap matters, stride < max_length:\n",
    "- more training examples\n",
    "- tokens appear in multiple, slightly shifted contexts\n",
    "- less context loss at chunk boundaries, also a big deal in RAG chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bc9d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T02:06:26.498638Z",
     "iopub.status.busy": "2026-02-23T02:06:26.498299Z",
     "iopub.status.idle": "2026-02-23T02:06:26.523280Z",
     "shell.execute_reply": "2026-02-23T02:06:26.521295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (320, 32)  (num_samples, seq_len)\n",
      "Y shape: (320, 32)\n",
      "\n",
      "Example sample #0\n",
      "input IDs : [40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438] ...\n",
      "target IDs: [367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568] ...\n",
      "\n",
      "input text : I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that,\n",
      "\n",
      "target text:  HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in\n"
     ]
    }
   ],
   "source": [
    "def build_windows(tokens, max_length=32, stride=16):\n",
    "    X, Y = [], []\n",
    "    last_start = len(tokens) - (max_length + 1)\n",
    "    for start in range(0, max(0, last_start + 1), stride):\n",
    "        chunk = tokens[start : start + max_length + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    return X, Y\n",
    "\n",
    "max_length = 32\n",
    "stride = 16\n",
    "\n",
    "X, Y = build_windows(token_ids, max_length=max_length, stride=stride)\n",
    "\n",
    "print(\"X shape:\", tuple(X.shape), \" (num_samples, seq_len)\")\n",
    "print(\"Y shape:\", tuple(Y.shape))\n",
    "\n",
    "print(\"\\nExample sample #0\")\n",
    "print(\"input IDs :\", X[0][:20].tolist(), \"...\")\n",
    "print(\"target IDs:\", Y[0][:20].tolist(), \"...\")\n",
    "print(\"\\ninput text :\", decode(X[0][:60].tolist()))\n",
    "print(\"\\ntarget text:\", decode(Y[0][:60].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e9398",
   "metadata": {},
   "source": [
    "## 4) DataLoader batching\n",
    "\n",
    "Batches make training and embedding computation efficient:\n",
    "- better GPU/CPU utilization\n",
    "- consistent tensor shapes\n",
    "- clean iteration for training loops\n",
    "\n",
    "Agentic systems use the same batching idea when embedding many chunks or reranking many candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9aeff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T02:06:26.527580Z",
     "iopub.status.busy": "2026-02-23T02:06:26.527260Z",
     "iopub.status.idle": "2026-02-23T02:06:26.539846Z",
     "shell.execute_reply": "2026-02-23T02:06:26.538139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X: torch.Size([4, 32])\n",
      "Batch Y: torch.Size([4, 32])\n",
      "\n",
      "Decoded batch[0] input (first ~250 chars):\n",
      "  latter's mysterious abdication. But no--for it was not till after that event that the _rose Dubarry_ drawing-rooms had begun to display ...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "xb, yb = next(iter(loader))\n",
    "print(\"Batch X:\", xb.shape)\n",
    "print(\"Batch Y:\", yb.shape)\n",
    "print(\"\\nDecoded batch[0] input (first ~250 chars):\\n\", decode(xb[0].tolist())[:250], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccca548",
   "metadata": {},
   "source": [
    "## 5) Embeddings: IDs to vectors\n",
    "\n",
    "The embedding matrix is a learned lookup table that maps each token ID to a dense vector\n",
    "\n",
    "### Why do embeddings encode meaning?\n",
    "Because they are optimized for the learning task next-token prediction\n",
    "Tokens that appear in similar contexts produce similar gradient signals, so their vectors become closer in space. Over time, the geometry of the embedding space reflects meaning as usefulness for prediction\n",
    "\n",
    "### How are embeddings related to NN concepts?\n",
    "An embedding layer is equivalent to a linear layer applied to a one‑hot vector:\n",
    "- one‑hot(token_id) has a single 1\n",
    "- multiplying by a weight matrix selects one row → that row is the embedding\n",
    "\n",
    "So embeddings are standard NN parameters learned by backprop weights with gradients, specialized for discrete IDs\n",
    "\n",
    "Transformers usually add positional embeddings so the model can use token order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e308b1b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T02:06:26.584325Z",
     "iopub.status.busy": "2026-02-23T02:06:26.583902Z",
     "iopub.status.idle": "2026-02-23T02:06:26.604019Z",
     "shell.execute_reply": "2026-02-23T02:06:26.602433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token vectors: torch.Size([4, 32, 64])\n",
      "Position vectors: torch.Size([1, 32, 64])\n",
      "Combined input: torch.Size([4, 32, 64])\n",
      "\n",
      "Same row via weight indexing: True\n",
      "Token ID example: 290\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "\n",
    "token_embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim).to(device)\n",
    "pos_embed = nn.Embedding(num_embeddings=max_length, embedding_dim=embed_dim).to(device)\n",
    "\n",
    "xb, yb = next(iter(loader))\n",
    "xb = xb.to(device)\n",
    "\n",
    "tok_vecs = token_embed(xb)                  # (B, T, D)\n",
    "pos_ids = torch.arange(xb.shape[1], device=device)\n",
    "pos_vecs = pos_embed(pos_ids)[None, :, :]   # (1, T, D)\n",
    "x_in = tok_vecs + pos_vecs                  # (B, T, D)\n",
    "\n",
    "print(\"Token vectors:\", tok_vecs.shape)\n",
    "print(\"Position vectors:\", pos_vecs.shape)\n",
    "print(\"Combined input:\", x_in.shape)\n",
    "\n",
    "some_id = xb[0, 0].item()\n",
    "row_from_weight = token_embed.weight[some_id]\n",
    "row_from_lookup = token_embed(torch.tensor([some_id], device=device))[0]\n",
    "\n",
    "print(\"\\nSame row via weight indexing:\", torch.allclose(row_from_weight, row_from_lookup))\n",
    "print(\"Token ID example:\", some_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4eab76",
   "metadata": {},
   "source": [
    "## 6) Experiment: change max_length & stride\n",
    "\n",
    "We will vary max_length and stride and report how many samples are created\n",
    "\n",
    "Key idea:\n",
    "- smaller stride to more overlap to more samples\n",
    "- overlap is useful because it exposes tokens to multiple contexts and reduces boundary effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4229d368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T02:06:26.607556Z",
     "iopub.status.busy": "2026-02-23T02:06:26.607250Z",
     "iopub.status.idle": "2026-02-23T02:06:26.624679Z",
     "shell.execute_reply": "2026-02-23T02:06:26.623138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5145\n",
      "\n",
      "max_length | stride | num_samples\n",
      "--------------------------------\n",
      "       16 |     16 |         321\n",
      "       16 |      8 |         642\n",
      "       32 |     32 |         160\n",
      "       32 |     16 |         320\n",
      "       64 |     64 |          80\n",
      "       64 |     16 |         318\n",
      "\n",
      "Why overlap is useful:\n",
      "Overlap means each token appears in multiple training contexts, so the model learns smoother transitions across window boundaries.\n"
     ]
    }
   ],
   "source": [
    "def count_samples(tokens, max_length, stride):\n",
    "    X, _ = build_windows(tokens, max_length=max_length, stride=stride)\n",
    "    return X.shape[0]\n",
    "\n",
    "settings = [\n",
    "    (16, 16),  # no overlap\n",
    "    (16, 8),   # 50% overlap\n",
    "    (32, 32),  # no overlap\n",
    "    (32, 16),  # 50% overlap\n",
    "    (64, 64),  # no overlap\n",
    "    (64, 16),  # heavy overlap\n",
    "]\n",
    "\n",
    "print(\"Total tokens:\", len(token_ids))\n",
    "print(\"\\nmax_length | stride | num_samples\")\n",
    "print(\"-\" * 32)\n",
    "for ml, st in settings:\n",
    "    n = count_samples(token_ids, ml, st)\n",
    "    print(f\"{ml:9d} | {st:6d} | {n:11d}\")\n",
    "\n",
    "print(\"\\nWhy overlap is useful:\")\n",
    "print(\"Overlap means each token appears in multiple training contexts, so the model learns smoother transitions across window boundaries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
